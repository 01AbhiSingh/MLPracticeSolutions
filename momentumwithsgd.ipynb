{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport numpy as np\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-11T05:50:57.512678Z","iopub.execute_input":"2025-06-11T05:50:57.512972Z","iopub.status.idle":"2025-06-11T05:50:57.517728Z","shell.execute_reply.started":"2025-06-11T05:50:57.512951Z","shell.execute_reply":"2025-06-11T05:50:57.516772Z"}},"outputs":[],"execution_count":66},{"cell_type":"code","source":"class LinearRegression(nn.Module):\n    def __init__(self, input_features):\n        super().__init__()\n        self.weights = nn.Parameter(torch.randn(input_features, 1))\n        self.bias = nn.Parameter(torch.zeros(1))\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return torch.matmul(x, self.weights) + self.bias","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T05:50:57.519030Z","iopub.execute_input":"2025-06-11T05:50:57.519353Z","iopub.status.idle":"2025-06-11T05:50:57.537556Z","shell.execute_reply.started":"2025-06-11T05:50:57.519318Z","shell.execute_reply":"2025-06-11T05:50:57.536565Z"}},"outputs":[],"execution_count":67},{"cell_type":"code","source":"class MSELoss:\n    def __call__(self, y_true, y_pred):\n        return torch.mean((y_true - y_pred)**2)\n\n    def compute_gradients(self, x, y_true, y_pred):\n        N = x.shape[0]\n        error = y_true - y_pred\n        \n        grad_weights = -2/N * torch.matmul(x.T, error)\n        grad_bias = -2/N * torch.sum(error)\n        return grad_weights, grad_bias","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T05:50:57.538922Z","iopub.execute_input":"2025-06-11T05:50:57.539222Z","iopub.status.idle":"2025-06-11T05:50:57.558178Z","shell.execute_reply.started":"2025-06-11T05:50:57.539196Z","shell.execute_reply":"2025-06-11T05:50:57.557244Z"}},"outputs":[],"execution_count":68},{"cell_type":"code","source":"class MomentumOptimizer:\n    def __init__(self, parameters, learning_rate=0.01, momentum_coefficient=0.9):\n        self.parameters = list(parameters)\n        self.learning_rate = learning_rate\n        self.momentum_coefficient = momentum_coefficient\n        self.velocities = [torch.zeros_like(p) for p in self.parameters]\n\n    def step(self, gradients):\n        if len(self.parameters) != len(gradients):\n            raise ValueError(\"Number of parameters and gradients must match.\")\n\n        for i, (param, grad) in enumerate(zip(self.parameters, gradients)):\n            self.velocities[i] = (self.momentum_coefficient * self.velocities[i] +\n                                  self.learning_rate * grad)\n            param.data -= self.velocities[i]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T05:50:57.559005Z","iopub.execute_input":"2025-06-11T05:50:57.559227Z","iopub.status.idle":"2025-06-11T05:50:57.583816Z","shell.execute_reply.started":"2025-06-11T05:50:57.559210Z","shell.execute_reply":"2025-06-11T05:50:57.583101Z"}},"outputs":[],"execution_count":69},{"cell_type":"code","source":"def train_model(model, x_train, y_train, x_val, y_val,\n                          loss_fn, optimizer, num_epochs, batch_size):\n    num_samples = x_train.shape[0]\n\n    for epoch in range(num_epochs):\n        permutation = torch.randperm(num_samples)\n        x_shuffled = x_train[permutation]\n        y_shuffled = y_train[permutation]\n\n        for i in range(0, num_samples, batch_size):\n            x_batch = x_shuffled[i:i+batch_size]\n            y_batch = y_shuffled[i:i+batch_size]\n\n            y_pred = model.forward(x_batch)\n            \n            grad_weights, grad_bias = loss_fn.compute_gradients(x_batch, y_batch, y_pred)\n            gradients = [grad_weights, grad_bias]\n\n            optimizer.step(gradients)\n\n        train_loss = loss_fn(y_train, model.forward(x_train))\n        val_loss = loss_fn(y_val, model.forward(x_val))\n\n        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T05:50:57.585269Z","iopub.execute_input":"2025-06-11T05:50:57.585662Z","iopub.status.idle":"2025-06-11T05:50:57.603824Z","shell.execute_reply.started":"2025-06-11T05:50:57.585632Z","shell.execute_reply":"2025-06-11T05:50:57.603004Z"}},"outputs":[],"execution_count":70},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    np.random.seed(42)\n    num_samples = 1000\n    input_features = 2\n    X = np.random.rand(num_samples, input_features) * 10\n    true_weights = np.array([[2.5], [-1.5]])\n    true_bias = np.array([5.0])\n    y = np.dot(X, true_weights) + true_bias + np.random.randn(num_samples, 1) * 2\n\n    split_ratio = 0.8\n    split_index = int(num_samples * split_ratio)\n    X_train_np, X_val_np = X[:split_index], X[split_index:]\n    y_train_np, y_val_np = y[:split_index], y[split_index:]\n\n    X_train = torch.tensor(X_train_np, dtype=torch.float32)\n    y_train = torch.tensor(y_train_np, dtype=torch.float32)\n    X_val = torch.tensor(X_val_np, dtype=torch.float32)\n    y_val = torch.tensor(y_val_np, dtype=torch.float32)\n\n    model = LinearRegression(input_features)\n    loss_fn = MSELoss()\n    \n    optimizer = MomentumOptimizer(parameters=[model.weights, model.bias],\n                                  learning_rate=0.001,\n                                  momentum_coefficient=0.9)\n\n    num_epochs = 200\n    batch_size = 32\n    print(\"Starting training...\")\n    train_model(model, X_train, y_train, X_val, y_val,\n                              loss_fn, optimizer, num_epochs, batch_size)\n\n    print(\"\\nTraining finished.\")\n    print(\"Learned Weights:\", model.weights.flatten())\n    print(\"Learned Bias:\", model.bias[0])\n    print(\"True Weights:\", true_weights.flatten())\n    print(\"True Bias:\", true_bias[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T05:50:57.604905Z","iopub.execute_input":"2025-06-11T05:50:57.605737Z","iopub.status.idle":"2025-06-11T05:50:58.520572Z","shell.execute_reply.started":"2025-06-11T05:50:57.605713Z","shell.execute_reply":"2025-06-11T05:50:58.519638Z"}},"outputs":[{"name":"stdout","text":"Starting training...\nEpoch 1/200, Train Loss: 28.5707, Val Loss: 30.0011\nEpoch 2/200, Train Loss: 6.5866, Val Loss: 7.0309\nEpoch 3/200, Train Loss: 6.1730, Val Loss: 6.5312\nEpoch 4/200, Train Loss: 6.1608, Val Loss: 6.7087\nEpoch 5/200, Train Loss: 5.5742, Val Loss: 6.0088\nEpoch 6/200, Train Loss: 5.2785, Val Loss: 5.6592\nEpoch 7/200, Train Loss: 5.4317, Val Loss: 5.9211\nEpoch 8/200, Train Loss: 4.9257, Val Loss: 5.2940\nEpoch 9/200, Train Loss: 4.7660, Val Loss: 5.1180\nEpoch 10/200, Train Loss: 4.6290, Val Loss: 4.8964\nEpoch 11/200, Train Loss: 4.5039, Val Loss: 4.7637\nEpoch 12/200, Train Loss: 4.4921, Val Loss: 4.7040\nEpoch 13/200, Train Loss: 4.3659, Val Loss: 4.6736\nEpoch 14/200, Train Loss: 4.3106, Val Loss: 4.5036\nEpoch 15/200, Train Loss: 4.1872, Val Loss: 4.4338\nEpoch 16/200, Train Loss: 4.1339, Val Loss: 4.3636\nEpoch 17/200, Train Loss: 4.1485, Val Loss: 4.3202\nEpoch 18/200, Train Loss: 4.1709, Val Loss: 4.4720\nEpoch 19/200, Train Loss: 4.0235, Val Loss: 4.2293\nEpoch 20/200, Train Loss: 4.0886, Val Loss: 4.2305\nEpoch 21/200, Train Loss: 4.0173, Val Loss: 4.1318\nEpoch 22/200, Train Loss: 3.9708, Val Loss: 4.1150\nEpoch 23/200, Train Loss: 4.1153, Val Loss: 4.1774\nEpoch 24/200, Train Loss: 3.9140, Val Loss: 4.1167\nEpoch 25/200, Train Loss: 3.9294, Val Loss: 4.0354\nEpoch 26/200, Train Loss: 3.8992, Val Loss: 4.1021\nEpoch 27/200, Train Loss: 3.9260, Val Loss: 4.1312\nEpoch 28/200, Train Loss: 3.8687, Val Loss: 4.0360\nEpoch 29/200, Train Loss: 3.8788, Val Loss: 4.0590\nEpoch 30/200, Train Loss: 3.8535, Val Loss: 4.0274\nEpoch 31/200, Train Loss: 3.9050, Val Loss: 4.1086\nEpoch 32/200, Train Loss: 3.8915, Val Loss: 3.9879\nEpoch 33/200, Train Loss: 3.8929, Val Loss: 4.0748\nEpoch 34/200, Train Loss: 3.8741, Val Loss: 4.0712\nEpoch 35/200, Train Loss: 3.9672, Val Loss: 4.0165\nEpoch 36/200, Train Loss: 3.8163, Val Loss: 3.9295\nEpoch 37/200, Train Loss: 3.9011, Val Loss: 3.9538\nEpoch 38/200, Train Loss: 3.8449, Val Loss: 4.0151\nEpoch 39/200, Train Loss: 3.8146, Val Loss: 3.9493\nEpoch 40/200, Train Loss: 4.0194, Val Loss: 4.0409\nEpoch 41/200, Train Loss: 3.9395, Val Loss: 4.1293\nEpoch 42/200, Train Loss: 3.9266, Val Loss: 4.1260\nEpoch 43/200, Train Loss: 4.2469, Val Loss: 4.1989\nEpoch 44/200, Train Loss: 3.8251, Val Loss: 3.9617\nEpoch 45/200, Train Loss: 3.8658, Val Loss: 3.9389\nEpoch 46/200, Train Loss: 3.8402, Val Loss: 3.9872\nEpoch 47/200, Train Loss: 3.8056, Val Loss: 3.9235\nEpoch 48/200, Train Loss: 3.8206, Val Loss: 3.8958\nEpoch 49/200, Train Loss: 3.8147, Val Loss: 3.8954\nEpoch 50/200, Train Loss: 4.0237, Val Loss: 4.2378\nEpoch 51/200, Train Loss: 3.9134, Val Loss: 4.0863\nEpoch 52/200, Train Loss: 4.0292, Val Loss: 4.0367\nEpoch 53/200, Train Loss: 3.7986, Val Loss: 3.8958\nEpoch 54/200, Train Loss: 3.9474, Val Loss: 3.9628\nEpoch 55/200, Train Loss: 3.8051, Val Loss: 3.8978\nEpoch 56/200, Train Loss: 3.8035, Val Loss: 3.9066\nEpoch 57/200, Train Loss: 4.0648, Val Loss: 4.2934\nEpoch 58/200, Train Loss: 4.2333, Val Loss: 4.1877\nEpoch 59/200, Train Loss: 4.0224, Val Loss: 4.2248\nEpoch 60/200, Train Loss: 3.7994, Val Loss: 3.8888\nEpoch 61/200, Train Loss: 3.7998, Val Loss: 3.8889\nEpoch 62/200, Train Loss: 3.8623, Val Loss: 3.8917\nEpoch 63/200, Train Loss: 3.7966, Val Loss: 3.9005\nEpoch 64/200, Train Loss: 3.9518, Val Loss: 4.1470\nEpoch 65/200, Train Loss: 3.8011, Val Loss: 3.9224\nEpoch 66/200, Train Loss: 3.8049, Val Loss: 3.8870\nEpoch 67/200, Train Loss: 3.8208, Val Loss: 3.8936\nEpoch 68/200, Train Loss: 3.8786, Val Loss: 4.0375\nEpoch 69/200, Train Loss: 3.8288, Val Loss: 3.9690\nEpoch 70/200, Train Loss: 3.9500, Val Loss: 4.1467\nEpoch 71/200, Train Loss: 3.8006, Val Loss: 3.8978\nEpoch 72/200, Train Loss: 3.8656, Val Loss: 4.0277\nEpoch 73/200, Train Loss: 3.8612, Val Loss: 4.0080\nEpoch 74/200, Train Loss: 3.8023, Val Loss: 3.8901\nEpoch 75/200, Train Loss: 3.8763, Val Loss: 3.8953\nEpoch 76/200, Train Loss: 4.1938, Val Loss: 4.4482\nEpoch 77/200, Train Loss: 3.8215, Val Loss: 3.9667\nEpoch 78/200, Train Loss: 3.8681, Val Loss: 3.8955\nEpoch 79/200, Train Loss: 3.9109, Val Loss: 3.9292\nEpoch 80/200, Train Loss: 3.8069, Val Loss: 3.9322\nEpoch 81/200, Train Loss: 3.7973, Val Loss: 3.8997\nEpoch 82/200, Train Loss: 3.8425, Val Loss: 3.9897\nEpoch 83/200, Train Loss: 3.8418, Val Loss: 3.9362\nEpoch 84/200, Train Loss: 4.0906, Val Loss: 4.0988\nEpoch 85/200, Train Loss: 3.8742, Val Loss: 3.8943\nEpoch 86/200, Train Loss: 3.8577, Val Loss: 4.0169\nEpoch 87/200, Train Loss: 3.8915, Val Loss: 3.9114\nEpoch 88/200, Train Loss: 4.3971, Val Loss: 4.6839\nEpoch 89/200, Train Loss: 4.1045, Val Loss: 4.0696\nEpoch 90/200, Train Loss: 3.8131, Val Loss: 3.9160\nEpoch 91/200, Train Loss: 3.9628, Val Loss: 4.1651\nEpoch 92/200, Train Loss: 3.8131, Val Loss: 3.9418\nEpoch 93/200, Train Loss: 3.8539, Val Loss: 3.9054\nEpoch 94/200, Train Loss: 3.8316, Val Loss: 3.9698\nEpoch 95/200, Train Loss: 3.7978, Val Loss: 3.8876\nEpoch 96/200, Train Loss: 3.8522, Val Loss: 4.0124\nEpoch 97/200, Train Loss: 3.7971, Val Loss: 3.8885\nEpoch 98/200, Train Loss: 3.8939, Val Loss: 4.0661\nEpoch 99/200, Train Loss: 3.8175, Val Loss: 3.8806\nEpoch 100/200, Train Loss: 3.8082, Val Loss: 3.8846\nEpoch 101/200, Train Loss: 3.8088, Val Loss: 3.9353\nEpoch 102/200, Train Loss: 3.8129, Val Loss: 3.8994\nEpoch 103/200, Train Loss: 3.8114, Val Loss: 3.9299\nEpoch 104/200, Train Loss: 3.8658, Val Loss: 3.9031\nEpoch 105/200, Train Loss: 3.8479, Val Loss: 3.9982\nEpoch 106/200, Train Loss: 3.8191, Val Loss: 3.8926\nEpoch 107/200, Train Loss: 3.7975, Val Loss: 3.8849\nEpoch 108/200, Train Loss: 3.7969, Val Loss: 3.9017\nEpoch 109/200, Train Loss: 3.7999, Val Loss: 3.8881\nEpoch 110/200, Train Loss: 3.8501, Val Loss: 3.8908\nEpoch 111/200, Train Loss: 3.7992, Val Loss: 3.9134\nEpoch 112/200, Train Loss: 3.8262, Val Loss: 3.9626\nEpoch 113/200, Train Loss: 3.8300, Val Loss: 3.8894\nEpoch 114/200, Train Loss: 3.8095, Val Loss: 3.9384\nEpoch 115/200, Train Loss: 3.8214, Val Loss: 3.8770\nEpoch 116/200, Train Loss: 3.8039, Val Loss: 3.9275\nEpoch 117/200, Train Loss: 3.8015, Val Loss: 3.8810\nEpoch 118/200, Train Loss: 3.8483, Val Loss: 3.9986\nEpoch 119/200, Train Loss: 3.8036, Val Loss: 3.9228\nEpoch 120/200, Train Loss: 3.8410, Val Loss: 3.8815\nEpoch 121/200, Train Loss: 3.8853, Val Loss: 4.0544\nEpoch 122/200, Train Loss: 3.9758, Val Loss: 3.9651\nEpoch 123/200, Train Loss: 3.8584, Val Loss: 3.8998\nEpoch 124/200, Train Loss: 3.8457, Val Loss: 3.8812\nEpoch 125/200, Train Loss: 4.2921, Val Loss: 4.5467\nEpoch 126/200, Train Loss: 3.8052, Val Loss: 3.9242\nEpoch 127/200, Train Loss: 3.7995, Val Loss: 3.9036\nEpoch 128/200, Train Loss: 3.8199, Val Loss: 3.9408\nEpoch 129/200, Train Loss: 3.9050, Val Loss: 4.0793\nEpoch 130/200, Train Loss: 3.8285, Val Loss: 3.9597\nEpoch 131/200, Train Loss: 3.8024, Val Loss: 3.9188\nEpoch 132/200, Train Loss: 3.8084, Val Loss: 3.8727\nEpoch 133/200, Train Loss: 3.8154, Val Loss: 3.8854\nEpoch 134/200, Train Loss: 3.8966, Val Loss: 4.0624\nEpoch 135/200, Train Loss: 3.8602, Val Loss: 3.8897\nEpoch 136/200, Train Loss: 3.7982, Val Loss: 3.8898\nEpoch 137/200, Train Loss: 3.8213, Val Loss: 3.9547\nEpoch 138/200, Train Loss: 3.9794, Val Loss: 3.9768\nEpoch 139/200, Train Loss: 3.7982, Val Loss: 3.9080\nEpoch 140/200, Train Loss: 3.8014, Val Loss: 3.9033\nEpoch 141/200, Train Loss: 3.8357, Val Loss: 3.8960\nEpoch 142/200, Train Loss: 3.8459, Val Loss: 3.8997\nEpoch 143/200, Train Loss: 3.7968, Val Loss: 3.8995\nEpoch 144/200, Train Loss: 3.7968, Val Loss: 3.8905\nEpoch 145/200, Train Loss: 3.7974, Val Loss: 3.8844\nEpoch 146/200, Train Loss: 3.8128, Val Loss: 3.8952\nEpoch 147/200, Train Loss: 3.9272, Val Loss: 3.9214\nEpoch 148/200, Train Loss: 3.8042, Val Loss: 3.9271\nEpoch 149/200, Train Loss: 3.8086, Val Loss: 3.8749\nEpoch 150/200, Train Loss: 3.8473, Val Loss: 3.9884\nEpoch 151/200, Train Loss: 3.8595, Val Loss: 3.8962\nEpoch 152/200, Train Loss: 3.7977, Val Loss: 3.8913\nEpoch 153/200, Train Loss: 3.8828, Val Loss: 4.0550\nEpoch 154/200, Train Loss: 3.7964, Val Loss: 3.8954\nEpoch 155/200, Train Loss: 4.0024, Val Loss: 4.1955\nEpoch 156/200, Train Loss: 3.8548, Val Loss: 3.8945\nEpoch 157/200, Train Loss: 4.1031, Val Loss: 4.3229\nEpoch 158/200, Train Loss: 3.9149, Val Loss: 3.9392\nEpoch 159/200, Train Loss: 3.8888, Val Loss: 3.9192\nEpoch 160/200, Train Loss: 3.9083, Val Loss: 4.0900\nEpoch 161/200, Train Loss: 3.8905, Val Loss: 4.0638\nEpoch 162/200, Train Loss: 3.7968, Val Loss: 3.8943\nEpoch 163/200, Train Loss: 4.0394, Val Loss: 4.2466\nEpoch 164/200, Train Loss: 3.8021, Val Loss: 3.9221\nEpoch 165/200, Train Loss: 3.7999, Val Loss: 3.8842\nEpoch 166/200, Train Loss: 3.8510, Val Loss: 3.8807\nEpoch 167/200, Train Loss: 3.8017, Val Loss: 3.8929\nEpoch 168/200, Train Loss: 4.1149, Val Loss: 4.3371\nEpoch 169/200, Train Loss: 3.9645, Val Loss: 3.9689\nEpoch 170/200, Train Loss: 3.9719, Val Loss: 3.9732\nEpoch 171/200, Train Loss: 4.1881, Val Loss: 4.4381\nEpoch 172/200, Train Loss: 3.7986, Val Loss: 3.8821\nEpoch 173/200, Train Loss: 3.8144, Val Loss: 3.9493\nEpoch 174/200, Train Loss: 4.1438, Val Loss: 4.1071\nEpoch 175/200, Train Loss: 3.7982, Val Loss: 3.8836\nEpoch 176/200, Train Loss: 3.7979, Val Loss: 3.8914\nEpoch 177/200, Train Loss: 3.8042, Val Loss: 3.9102\nEpoch 178/200, Train Loss: 3.8347, Val Loss: 3.8791\nEpoch 179/200, Train Loss: 3.8506, Val Loss: 4.0066\nEpoch 180/200, Train Loss: 3.9188, Val Loss: 4.1113\nEpoch 181/200, Train Loss: 3.8000, Val Loss: 3.9148\nEpoch 182/200, Train Loss: 3.8368, Val Loss: 3.8820\nEpoch 183/200, Train Loss: 3.8530, Val Loss: 4.0021\nEpoch 184/200, Train Loss: 3.8464, Val Loss: 3.8824\nEpoch 185/200, Train Loss: 4.1905, Val Loss: 4.4362\nEpoch 186/200, Train Loss: 3.8164, Val Loss: 3.9357\nEpoch 187/200, Train Loss: 3.8396, Val Loss: 3.9956\nEpoch 188/200, Train Loss: 3.8674, Val Loss: 3.9049\nEpoch 189/200, Train Loss: 3.8013, Val Loss: 3.9171\nEpoch 190/200, Train Loss: 3.8105, Val Loss: 3.8714\nEpoch 191/200, Train Loss: 3.8014, Val Loss: 3.8840\nEpoch 192/200, Train Loss: 3.8255, Val Loss: 3.8788\nEpoch 193/200, Train Loss: 3.8026, Val Loss: 3.9229\nEpoch 194/200, Train Loss: 3.8591, Val Loss: 4.0267\nEpoch 195/200, Train Loss: 3.7990, Val Loss: 3.8872\nEpoch 196/200, Train Loss: 3.8075, Val Loss: 3.9307\nEpoch 197/200, Train Loss: 3.8051, Val Loss: 3.8882\nEpoch 198/200, Train Loss: 3.8061, Val Loss: 3.9121\nEpoch 199/200, Train Loss: 3.7985, Val Loss: 3.8985\nEpoch 200/200, Train Loss: 4.2534, Val Loss: 4.1985\n\nTraining finished.\nLearned Weights: tensor([ 2.4280, -1.5781], grad_fn=<ViewBackward0>)\nLearned Bias: tensor(5.1627, grad_fn=<SelectBackward0>)\nTrue Weights: [ 2.5 -1.5]\nTrue Bias: 5.0\n","output_type":"stream"}],"execution_count":71},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}